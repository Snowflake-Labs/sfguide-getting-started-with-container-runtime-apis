{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Quickstart Guide for XGBoost Training Using Snowflake's ML Container Runtime and OSS XGBoost\n",
    "## Introduction\n",
    "This notebook provides a quickstart for training an XGBoost model using Snowflake's ML Container Runtime APIs and OSS XGBoost. We demonstrate:\n",
    "1. Data ingestion from a Snowflake table.\n",
    "2. Splitting the data into train and test sets (90% train, 10% test).\n",
    "3. Training an XGBoost model using the OSS XGB.\n",
    "4. Scaling out to multiple GPUs using Snowflake Container Runtime APIs for distributed training.\n",
    "5. Making predictions with the trained models.\n",
    "\n",
    "### Steps Covered:\n",
    "- Load data from a Snowflake table.\n",
    "- Split the data for training and testing.\n",
    "- Train using both OSS and Snowflake Container Runtime APIs.\n",
    "- Make predictions and evaluate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "### Step 1: Set Up Snowflake Session\n",
    "Initialize a Snowflake session to perform operations within the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Snowflake session\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "### Step 2: Load Data from Snowflake Table\n",
    "We load data from the `CR_QUICKSTART.PUBLIC.VEHICLE` table and split it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the Snowflake table\n",
    "table_name = 'CR_QUICKSTART.PUBLIC.VEHICLE'\n",
    "snowpark_df = session.table(table_name)\n",
    "\n",
    "# Convert Snowpark DataFrame to Pandas DataFrame using DataConnector\n",
    "from snowflake.ml.data.data_connector import DataConnector\n",
    "pandas_df = DataConnector.from_dataframe(snowpark_df).to_pandas()\n",
    "\n",
    "# Drop the 'C2' column (datetime column) from the dataset\n",
    "pandas_df = pandas_df.drop(columns=['C2'])\n",
    "\n",
    "# Split the data into features (X) and target (y). Assume C6 is the target column.\n",
    "X = pandas_df.drop('C6', axis=1)\n",
    "y = pandas_df['C6']\n",
    "\n",
    "# Train-test split: 90% training and 10% testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]} rows\")\n",
    "print(f\"Test set size: {X_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oss-training",
   "metadata": {},
   "source": [
    "### Step 3: Train XGBoost Model Using OSS XGB\n",
    "We first train the model using the open-source XGBoost library (OSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OSS XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Train an XGBoost regressor using the OSS approach\n",
    "print('Training a sample XGBoost regressor using OSS solution')\n",
    "oss_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "oss_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oss-prediction",
   "metadata": {},
   "source": [
    "### Step 4: Make Predictions with the OSS Model\n",
    "We make predictions using the trained OSS model and display a sample of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the OSS model\n",
    "y_pred_oss = oss_model.predict(X_test)\n",
    "\n",
    "# Print sample predictions\n",
    "print(f'Sample predictions: {y_pred_oss[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "snowflake-api-training",
   "metadata": {},
   "source": [
    "### Step 5: Train Using Snowflake's Container Runtime APIs\n",
    "Leverage Snowflake's ML Container Runtime APIs to scale the training to multiple GPUs. The training APIs allow easy configuration of resources, including GPU and memory allocation.\n",
    "\n",
    "**Note:** By default, these APIs utilize all available resources, ensuring efficient use of hardware for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes for Snowflake Container Runtime training\n",
    "from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n",
    "\n",
    "# Set up the scaling configuration for multi-GPU usage\n",
    "scaling_config = XGBScalingConfig(\n",
    "    num_workers=-1,            # Use all available workers\n",
    "    num_cpu_per_worker=-1,     # Use all available CPU cores per worker\n",
    "    use_gpu=True               # Enable GPU for training\n",
    ")\n",
    "\n",
    "# Define the XGBEstimator for training\n",
    "estimator = XGBEstimator(\n",
    "    n_estimators=100,                     # Number of trees\n",
    "    objective='reg:squarederror',         # Objective function for regression\n",
    "    scaling_config=scaling_config         # Use GPU and multi-worker scaling\n",
    ")\n",
    "\n",
    "# Drop the 'C2' column from the Snowpark DataFrame for training\n",
    "snowpark_df = snowpark_df.drop(['C2'])\n",
    "\n",
    "# Define the label column and input columns\n",
    "label_col = 'C6'\n",
    "input_cols = [col for col in snowpark_df.columns if col != 'C6']\n",
    "\n",
    "# Split the Snowpark DataFrame into train and test sets\n",
    "train_snowpark_df = snowpark_df.sample(0.9)  # 90% for training\n",
    "test_snowpark_df = snowpark_df.subtract(train_snowpark_df)  # Remaining 10% for testing\n",
    "\n",
    "# Drop the label column from the test data to match the input structure for prediction\n",
    "test_snowpark_df = test_snowpark_df.drop([label_col])\n",
    "\n",
    "# Create DataConnectors for the training and testing sets\n",
    "train_data_connector = DataConnector.from_dataframe(train_snowpark_df)\n",
    "test_data_connector = DataConnector.from_dataframe(test_snowpark_df)\n",
    "\n",
    "# Train the model using Snowflake's Container Runtime API\n",
    "estimator.fit(\n",
    "    train_data_connector,   # Data for training\n",
    "    input_cols=input_cols,  # Input features (all columns except 'C6')\n",
    "    label_col=label_col     # Target column ('C6')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "snowflake-prediction",
   "metadata": {},
   "source": [
    "### Step 6: Make Predictions with the Snowflake Container Runtime Model\n",
    "We make predictions using the model trained with Snowflake's Container Runtime and display a sample of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the Snowflake Container Runtime model\n",
    "y_pred_snowflake = estimator.predict(test_data_connector)\n",
    "\n",
    "# Display the first 10 predictions\n",
    "print(f'Snowflake Container Runtime predictions: {y_pred_snowflake[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we demonstrated how to:\n",
    "- Load data from a Snowflake table.\n",
    "- Split the data into training and testing sets.\n",
    "- Train an XGBoost model using both the OSS approach and Snowflake Container Runtime APIs.\n",
    "- Make predictions with the trained models and compare performance.\n",
    "\n",
    "Snowflake's Container Runtime APIs provide an efficient way to leverage multi-GPU systems for distributed training, offering easy configuration for scaling resources. You can now adapt this notebook to your own datasets and models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
