{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "4ff59069",
            "metadata": {},
            "source": [
                "# Quickstart Guide for PyTorch Training Using Snowflake's ML Container Runtime\n",
                "## Introduction\n",
                "This notebook provides a quickstart for training a PyTorch model using Snowflake's ML Container Runtime APIs. We will use Snowflake's `ShardedDataConnector` and `PyTorchDistributor` for distributed training on a dataset using multiple GPUs.\n",
                "\n",
                "### Steps Covered:\n",
                "- Create and configure a compute pool with multi-GPU support.\n",
                "- Load data from a Snowflake table.\n",
                "- Set up Snowflake's `ShardedDataConnector` for data ingestion.\n",
                "- Train a PyTorch model leveraging multiple GPUs.\n",
                "- Make predictions or evaluate the model."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1c1bc687",
            "metadata": {},
            "source": [
                "### Step 1: Create a Compute Pool with Multi-GPU Support\n",
                "To enable multi-GPU distributed training, we need a compute pool with access to GPU instances. Run the following SQL command in your Snowflake console to create this pool:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0824648c",
            "metadata": {},
            "outputs": [],
            "source": [
                "-- Run this command in your Snowflake console to create a multi-GPU compute pool\n",
                "CREATE COMPUTE POOL QUICKSTART_GPU_POOL\n",
                "INSTANCE_FAMILY = 'GPU_NV_M'\n",
                "MIN_NODES = 1\n",
                "MAX_NODES = 2\n",
                "AUTO_SUSPEND_SECS = 3600;"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7bcc8ee4",
            "metadata": {},
            "source": [
                "After creating the compute pool, navigate to **Edit Compute Pool** in your Snowflake notebook, and select `QUICKSTART_GPU_POOL` as the compute pool for this session. This ensures that your training job runs on the newly created multi-GPU compute pool, supporting efficient distributed training. You can find more details about compute pool creation here(https://docs.snowflake.com/en/sql-reference/sql/create-compute-pool)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "647c7333",
            "metadata": {},
            "source": [
                "### Step 2: Set Up Snowflake Session\n",
                "Initialize a Snowflake session to perform operations within the environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a5835eb2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Snowflake session\n",
                "from snowflake.snowpark.context import get_active_session\n",
                "session = get_active_session()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "82d01bd4",
            "metadata": {},
            "source": [
                "### Step 3: Load Data from Snowflake Table\n",
                "We load data from the `CR_QUICKSTART.PUBLIC.VEHICLE` table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e8819ba6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data from the Snowflake table\n",
                "table_name = 'CR_QUICKSTART.PUBLIC.VEHICLE'\n",
                "snowpark_df = session.table(table_name)\n",
                "\n",
                "# Convert Snowpark DataFrame to Pandas DataFrame using DataConnector\n",
                "from snowflake.ml.data.data_connector import DataConnector\n",
                "pandas_df = DataConnector.from_dataframe(snowpark_df).to_pandas()\n",
                "\n",
                "# Drop the 'C2' column (datetime column) from the dataset\n",
                "pandas_df = pandas_df.drop(columns=['C2'])\n",
                "\n",
                "# Split the data into features (X) and target (y). Assume C6 is the target column.\n",
                "X = pandas_df.drop('C6', axis=1)\n",
                "y = pandas_df['C6']\n",
                "\n",
                "# Define input columns and label column\n",
                "input_cols = X.columns.tolist()\n",
                "label_col = 'C6'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f13c45b2",
            "metadata": {},
            "source": [
                "### Step 4: Set Up Snowflake ShardedDataConnector\n",
                "Use the `ShardedDataConnector` to ingest the dataset into the Snowflake environment for model training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71b3f633",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create ShardedDataConnector for data ingestion\n",
                "from snowflake.ml.data.sharded_data_connector import ShardedDataConnector\n",
                "\n",
                "# Drop the 'C2' column (datetime column) from the dataset\n",
                "snowpark_df = snowpark_df.drop(['C2'])\n",
                "data_connector = ShardedDataConnector.from_dataframe(snowpark_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e3a1525b",
            "metadata": {},
            "source": [
                "### Step 5: Define and Train the PyTorch Model\n",
                "We define a PyTorch model and configure it for distributed training using `PyTorchDistributor`. This configuration leverages multi-GPU support to optimize training performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "af37c33e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary PyTorch libraries\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "# Define a simple neural network\n",
                "class SimpleNet(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, output_size):\n",
                "        super(SimpleNet, self).__init__()\n",
                "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
                "        self.relu = nn.ReLU()\n",
                "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
                "    def forward(self, x):\n",
                "        x = self.fc1(x)\n",
                "        x = self.relu(x)\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "# Define the training function\n",
                "def train_func():\n",
                "    import torch.distributed as dist\n",
                "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
                "    from snowflake.ml.modeling.distributors.pytorch import get_context\n",
                "    context = get_context()\n",
                "    rank = context.get_rank()\n",
                "    dist.init_process_group(backend='gloo')\n",
                "\n",
                "    # Initialize model, loss function, and optimizer\n",
                "    model = SimpleNet(input_size=len(input_cols), hidden_size=32, output_size=1).to(rank)\n",
                "    model = DDP(model)\n",
                "    criterion = nn.MSELoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "    # Retrieve training data\n",
                "    dataset_map = context.get_dataset_map()\n",
                "    torch_dataset = dataset_map['train'].get_shard().to_torch_dataset(batch_size=1024)\n",
                "    dataloader = DataLoader(torch_dataset, batch_size=None)\n",
                "\n",
                "    # Training loop\n",
                "    for epoch in range(10):\n",
                "        for batch_dict in dataloader:   \n",
                "            labels = batch_dict.pop(label_col).squeeze().float().to(rank)\n",
                "            features = torch.stack(\n",
                "                    [tensor.to(torch.float32).squeeze() for key, tensor in batch_dict.items()], dim=1\n",
                "            ).to(rank)\n",
                "\n",
                "            output = model(features)\n",
                "            loss = criterion(output, labels.unsqueeze(1))\n",
                "\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "        print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')\n",
                "\n",
                "    print('Training finished')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7ee5f175",
            "metadata": {},
            "source": [
                "### Step 6: Run Distributed Training\n",
                "Configure and start the distributed training process using `PyTorchDistributor`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ad76713",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import and configure PyTorchDistributor for multi-GPU distributed training\n",
                "from snowflake.ml.modeling.distributors.pytorch import PyTorchDistributor\n",
                "from snowflake.ml.modeling.distributors.pytorch.scaling_config import (\n",
                "    PyTorchScalingConfig, WorkerResourceConfig\n",
                ")\n",
                "\n",
                "distributor = PyTorchDistributor(\n",
                "    train_func=train_func,  # Function that trains the model\n",
                "    scaling_config=PyTorchScalingConfig(num_nodes=1, num_workers_per_node=1, resource_requirements_per_worker=WorkerResourceConfig(num_cpus=0, num_gpus=1))  # Number of processes (adjust based on number of GPUs)\n",
                ")\n",
                "\n",
                "# Start training\n",
                "distributor.run(\n",
                "    dataset_map=dict(train=data_connector)\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "54b4bf1b",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "In this notebook, we demonstrated how to:\n",
                "- Set up a Snowflake session\n",
                "- Load and prepare data using a Snowflake `ShardedDataConnector`\n",
                "- Train a PyTorch model using the `PyTorchDistributor` API with multi-GPU support\n",
                "- Evaluate the model after training\n",
                "\n",
                "You can now apply these steps to your own datasets and models!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
