{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
       "# Quickstart Guide for PyTorch Training Using Snowflake's ML Container Runtime\n",
       "## Introduction\n",
       "This notebook provides a quickstart for training a PyTorch model using Snowflake's ML Container Runtime APIs. We will use Snowflake's `ShardedDataConnector` and `PyTorchDistributor` for distributed training on a dataset using multiple GPUs.\n",
       "\n",
       "### Steps Covered:\n",
       "- Load data from a Snowflake table.\n",
       "- Set up Snowflake's `ShardedDataConnector` for data ingestion.\n",
       "- Train a PyTorch model leveraging multiple GPUs (or fallback to CPU if GPUs are not available).\n",
       "- Make predictions or evaluate the model."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "step1",
      "metadata": {},
      "source": [
       "### Step 1: Set Up Snowflake Session\n",
       "Initialize a Snowflake session to perform operations within the environment."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell1",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize Snowflake session\n",
       "from snowflake.snowpark.context import get_active_session\n",
       "session = get_active_session()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "step2",
      "metadata": {},
      "source": [
       "### Step 2: Load Data from Snowflake Table\n",
       "We load data from the `CR_QUICKSTART.PUBLIC.VEHICLE` table."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell2",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load data from the Snowflake table\n",
       "table_name = 'CR_QUICKSTART.PUBLIC.VEHICLE'\n",
       "snowpark_df = session.table(table_name)\n",
       "\n",
       "# Convert Snowpark DataFrame to Pandas DataFrame using DataConnector\n",
       "from snowflake.ml.data.data_connector import DataConnector\n",
       "pandas_df = DataConnector.from_dataframe(snowpark_df).to_pandas()\n",
       "\n",
       "# Drop the 'C2' column (datetime column) from the dataset\n",
       "pandas_df = pandas_df.drop(columns=['C2'])\n",
       "\n",
       "# Split the data into features (X) and target (y). Assume C6 is the target column.\n",
       "X = pandas_df.drop('C6', axis=1)\n",
       "y = pandas_df['C6']\n",
       "\n",
       "# Define input columns and label column\n",
       "input_cols = X.columns.tolist()\n",
       "label_col = 'C6'"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "step3",
      "metadata": {},
      "source": [
       "### Step 3: Set Up Snowflake ShardedDataConnector\n",
       "Use the `ShardedDataConnector` to ingest the dataset into the Snowflake environment for model training."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell3",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create ShardedDataConnector for data ingestion\n",
       "from snowflake.ml.data.sharded_data_connector import ShardedDataConnector\n",
       "\n",
       "# Drop the 'C2' column (datetime column) from the dataset\n",
       "snowpark_df = snowpark_df.drop(['C2'])\n",
       "data_connector = ShardedDataConnector.from_dataframe(snowpark_df)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "step4",
      "metadata": {},
      "source": [
       "### Step 4: Define and Train the PyTorch Model\n",
       "We define a PyTorch model and configure it for distributed training using `PyTorchDistributor`. This configuration leverages multi-GPU support to optimize training performance."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell4",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Import necessary PyTorch libraries\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "from torch.utils.data import DataLoader\n",
       "\n",
       "# Define a simple neural network\n",
       "class SimpleNet(nn.Module):\n",
       "    def __init__(self, input_size, hidden_size, output_size):\n",
       "        super(SimpleNet, self).__init__()\n",
       "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
       "        self.relu = nn.ReLU()\n",
       "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
       "    def forward(self, x):\n",
       "        x = self.fc1(x)\n",
       "        x = self.relu(x)\n",
       "        x = self.fc2(x)\n",
       "        return x\n",
       "\n",
       "# Define the training function\n",
       "def train_func():\n",
       "    import torch.distributed as dist\n",
       "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
       "    from snowflake.ml.modeling.distributors.pytorch import get_context\n",
       "    context = get_context()\n",
       "    rank = context.get_rank()\n",
       "    dist.init_process_group(backend='gloo')\n",
       "\n",
       "    # Initialize model, loss function, and optimizer\n",
       "    model = SimpleNet(input_size=len(input_cols), hidden_size=32, output_size=1).to(rank)\n",
       "    model = DDP(model)\n",
       "    criterion = nn.MSELoss()\n",
       "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
       "\n",
       "    # Retrieve training data\n",
       "    dataset_map = context.get_dataset_map()\n",
       "    torch_dataset = dataset_map['train'].get_shard().to_torch_dataset(batch_size=1024)\n",
       "    dataloader = DataLoader(torch_dataset)\n",
       "\n",
       "    # Training loop\n",
       "    for epoch in range(10):\n",
       "        for batch_dict in dataloader:\n",
       "            features = torch.cat([batch_dict[col].T for col in input_cols], dim=1).float().to(rank)\n",
       "            labels = batch_dict[label_col].T.squeeze(0).float().to(rank)\n",
       "            output = model(features)\n",
       "            loss = criterion(output, labels.unsqueeze(1))\n",
       "\n",
       "            optimizer.zero_grad()\n",
       "            loss.backward()\n",
       "            optimizer.step()\n",
       "        print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')\n",
       "\n",
       "    print('Training finished')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "step5",
      "metadata": {},
      "source": [
       "### Step 5: Run Distributed Training\n",
       "Configure and start the distributed training process using `PyTorchDistributor`."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell5",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Set up PyTorchDistributor for distributed training\n",
       "from snowflake.ml.modeling.distributors.pytorch import PyTorchDistributor, PyTorchScalingConfig, WorkerResourceConfig\n",
       "\n",
       "pytorch_trainer = PyTorchDistributor(\n",
       "    train_func=train_func,\n",
       "    scaling_config=PyTorchScalingConfig(\n",
       "        num_nodes=1,\n",
       "        num_workers_per_node=2,\n",
       "        resource_requirements_per_worker=WorkerResourceConfig(num_cpus=0, num_gpus=1)\n",
       "    )\n",
       ")\n",
       "\n",
       "# Run the training process\n",
       "pytorch_trainer.run(dataset_map={'train': data_connector})"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "In this notebook, we demonstrated how to:\n",
       "- Set up a Snowflake session\n",
       "- Load and prepare data using a Snowflake `ShardedDataConnector`\n",
       "- Train a PyTorch model using the `PyTorchDistributor` API with multi-GPU support\n",
       "- Evaluate the model after training\n",
       "\n",
       "You can now apply these steps to your own datasets and models!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   